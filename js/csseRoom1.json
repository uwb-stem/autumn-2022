{
    "csse": [
        {
            "time": "1:00 PM - 1:15 PM",
            "projectId": "csse-1-100",
            "title": "Software Engineering Studio: Using Azure Devops to Teach the CI/CD Pipline In a Class Environment",
            "studentName": "Mark Mavis",
            "studentMajor": "CSSE",
            "projectType": "CSS Faculty Research",
            "facultyAdvisor": "Mr. Mark Kochanski",
            "posterLink": "./posters/csse/mavis-mark-updated.jpg",
            "abstract": "The focus of my work was on the development and improvement of an early-stage CSS curriculum course titled “Software Engineering Studio” that aims to give university software engineers exposure to the continuous integration and continuous development pipeline using Microsoft’s Azure DevOps cloud tool. Members of SES will use the DevOps platform to track and execute all work item management and push code to live deployed systems. A variety of previously implemented technologies are introduced in the form of repositories that students can clone to their own systems for further exploration and testing. The curriculum reinforces students’ experience with guided walkthroughs and tutorials that give them clear goals to research and implement their own features that will be merged into the larger system.\n\nMy role in the project was to join and complete the SES course as a student and use that experience to find ways that the SES experience might be able to be improved over the course of two quarters. As a student, I took note of elements of the existing project architecture that I felt got in the way of my understanding or comprehension. Because the course focus is on the CI/CD pipeline itself, all my work pertained specifically to Azure DevOps organization and the improvement of the cloud architecture it provides to support the software development process, which in our case is in a classroom environment.\n\n  The purpose of my work was to first architect persistent structures and organizations that could be depended on to improve efficiency and manageability. By creating a DevOps hierarchy that provides consistency quarter to quarter, we can generate a network of automation to self-govern projects, and the teams that are developing for those projects. Secondly, I was to implement a series of analytics dashboards and supporting queries that administrators and team members could use to improve their work process and the health of the projects. The major hurdle of this project that will need to be addressed is to go against the tradition “top down” method of work items being handed down to teams and create a structure that can accommodate a “bottom up” process where teams operate autonomously from management. The difference between the two is that the top-down method plans on the teams being “siloed” from each other and no visibility to each other and the bottom-up method requires teams to gain visibility to each other to not duplicate work. My focus is to develop a work around to this issue using ADO.\n\nI was able to design, build, and implement a complete re-organization of the Azure DevOps structure to support all established objectives previously stated.  All native analytics functionality provided by ADO was able to be recovered, including a global iteration path that all ADO work item tracking structures can follow synchronously. My architecture allows teams to create work items in their own respective works areas to allow admins to track detailed team focused data and project focused data. I instituted two “team” structures that allow teams to gain visibility to the other teams around them to improve coordination and eliminate work duplication. I was able to also set up a system of automation that can be templated and duplicated to allow for future features and analytics. Overall, the project was a complete success in terms of goals and objectives.\n\nI believe that this project represents a significant opportunity in our ability to expose early software developers to a wide range of technologies that they themselves would otherwise, really have no ability to interact with.  Generating a growing volume of repositories representing a variety of technologies would be a powerful resource for CSS students to gain hands-on experience with these systems and the way they are being implemented. This project represents a major opportunity for CSS departments across the world to expand the digital capital they can offer their students."
        },
        {
            "time": "1:15 PM - 1:30 PM",
            "projectId": "csse-1-115",
            "title": "Data Engineering Internship at Ammex",
            "studentName": "Michael Le",
            "studentMajor": "CSSE",
            "projectType": "Internship - Ammex Corp.",
            "facultyAdvisor": "Mr. Mark Kochanski",
            "posterLink": "./posters/csse/le-michael.jpg",
            "abstract": "For my capstone work, it was a data engineering internship at Ammex. In the IT department, specifically the data engineers, their job is building the data warehouse, creating ETL pipelines within Microsoft Azure that can ingest data that will then be turned into data that will be used for business decisions. The ETL pipelines that were built by the other data engineers would have issues, such as long runtime or would run into specific errors. My work during the internship was to create an ETL pipeline(s) that would collect all the pipeline information to be able to troubleshoot / increase the efficiency of the pipelines.\n\nFirst, I created a test pipeline that I was able to edit and run however much I like. Within this test pipeline, I decided to make multiple separate pipelines that run different REST APIs that collected different information. I created multiple smaller pipelines to be able to troubleshoot or debug the pipelines within the process. Once I created this pipeline, I had to do troubleshooting to decrease the amount of time it takes to receive this information, since my first prototype would run for 6 hours. After everything was finished, I moved my material from the test pipeline to the production pipeline, which required renaming all of the pipelines to match the naming conventions of the data warehouse.\n\nAfter I created a pipeline that created this information, I used Logic App to create an emailing system to notify other data engineers what pipelines/activities passed and failed overnight so that the engineers can receive the email in the morning to troubleshoot before continuing with other work. On top of the emailing system, I received assistance from another engineer in creating a PowerBI dashboard to see the success and failure rate of pipelines every night.\n\nMy work was very important due to the fact that when pipelines are run, Azure charges the company for any amount of run time. When pipelines are run, and they take too long or fail, it will cost the company to pay for those mistakes. Using my project, the other engineers are able to troubleshoot failing / long run time pipelines to be able to reduce the costs of Azure."
        }                  
    ]
}